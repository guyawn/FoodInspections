{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BDWXPWwZC5E0"
   },
   "source": [
    "Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjDCgDiXWWaO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle\n",
    "import cv2\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IpYKJwUBDCn9"
   },
   "source": [
    "Load the zipped data file of the specified type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eauyrJE109CB"
   },
   "outputs": [],
   "source": [
    "dataset = \"BOW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aj7rWLcKyFaZ"
   },
   "outputs": [],
   "source": [
    "if not(os.path.exists(dataset)):\n",
    "  with zipfile.ZipFile(dataset + \".zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBIGyikhZTO2"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(dataset + \"/X.pkl\", \"rb\") as f:\n",
    "  X = pickle.load(f)\n",
    "\n",
    "with open(dataset + \"/y.pkl\", \"rb\") as f:\n",
    "  y = pickle.load(f)\n",
    "\n",
    "X = X.to('cuda').float()\n",
    "y = y.to('cuda').float()\n",
    "y = (y> 10).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9MAGMmOXmSQ"
   },
   "outputs": [],
   "source": [
    "indices = np.arange(0, X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:(X.shape[0] - 1000)]\n",
    "test_indices = indices[(X.shape[0] - 1000):]\n",
    "\n",
    "y_train = y[train_indices]\n",
    "y_test = y[test_indices]\n",
    "X_train = X[train_indices, :, :]\n",
    "X_test = X[test_indices, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3aMGsMODLWR"
   },
   "source": [
    "Define an RNN architecture for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMCGDjC7Zdhf"
   },
   "outputs": [],
   "source": [
    "# Define the network as a class\n",
    "class RNNNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(RNNNetwork, self).__init__()\n",
    "        \n",
    "        #  Recurrent (sequence) layer\n",
    "        self.lstm = torch.nn.LSTM(input_size, 25)\n",
    "\n",
    "        # Two linear layers\n",
    "        self.linear1 = torch.nn.Linear(100, 25)\n",
    "        self.linear2 = torch.nn.Linear(25, 1)\n",
    "\n",
    "        # Dropout of 50% of values\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "        # Sigmoid activation\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    # Performs the forward pass\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        #out = self.linear1(out.view(len(x), -1))\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out[len(x)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZ1Hoez6LcGY"
   },
   "outputs": [],
   "source": [
    "# Declare a model and prepare for training\n",
    "rnn_model = RNNNetwork(X_train.shape[2])\n",
    "rnn_model.to('cuda')\n",
    "rnn_model.train(True)\n",
    "rnn_model.float()\n",
    "\n",
    "# Use the mse loss for a criterion\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the adam optimizer on the discriminator network\n",
    "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.000001,\n",
    "                                 betas=(0.9, 0.9999))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJ7wHz_TDcnd"
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "colab_type": "code",
    "id": "_OzioBvgMaqc",
    "outputId": "da91fa7d-8d6d-4963-c725-0b815e270bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  of  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10  of  100\n",
      "20  of  100\n",
      "30  of  100\n",
      "40  of  100\n",
      "50  of  100\n",
      "60  of  100\n",
      "70  of  100\n",
      "80  of  100\n",
      "90  of  100\n",
      "100  of  100\n"
     ]
    }
   ],
   "source": [
    "# Save the losses\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "epochs = 100\n",
    "\n",
    "# Use  100 epochs\n",
    "for i in range(0, epochs+1):\n",
    "    \n",
    "    # Every tenth iteration\n",
    "    if (i % 10 == 0):\n",
    "\n",
    "      # Track the epoch count\n",
    "      print(i, \" of \", epochs)\n",
    "\n",
    "      # Get loss on the training data\n",
    "      epoch_losses_train = []\n",
    "      for j in range(0, int(X_train.shape[0])):\n",
    "      \n",
    "          # Zero the  gradient\n",
    "          rnn_optimizer.zero_grad()\n",
    "\n",
    "          # Select inputs\n",
    "          train_input = X_train[j:(j+1),:, :]\n",
    "          train_input = X_train[(j):(j+1),:((np.where(train_input.sum(axis=2).cpu().numpy() != -300))[1]).max(), :]\n",
    "          train_input = train_input.reshape(train_input.shape[1], 1, train_input.shape[2])\n",
    "\n",
    "          # Skip this input if needed\n",
    "          if train_input.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "          # Get outputs and truth\n",
    "          train_output = rnn_model(train_input)\n",
    "          true_output = y_train[j:(j+1)]\n",
    "          \n",
    "          # Calculate the loss and update the model\n",
    "          loss = criterion(train_output, true_output)\n",
    "          loss.backward()\n",
    "          rnn_optimizer.step()\n",
    "\n",
    "          # Save the loss for this training iteration\n",
    "          epoch_losses_train.append(loss)\n",
    "\n",
    "      # Set training to false\n",
    "      rnn_model.train(False)\n",
    "\n",
    "      # Get loss on the testing data\n",
    "      epoch_losses_test = []\n",
    "      for j in range(0, int(X_test.shape[0])):\n",
    "\n",
    "          # Select inputs\n",
    "          train_input = X_test[j:(j+1),:, :]\n",
    "          train_input = X_test[(j):(j+1),:((np.where(train_input.sum(axis=2).cpu().numpy() != -300))[1]).max(), :]\n",
    "          train_input = train_input.reshape(train_input.shape[1], 1, train_input.shape[2])\n",
    "\n",
    "          # Sip if neceeded\n",
    "          if train_input.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "          # Get outputs and truth\n",
    "          train_output = rnn_model(train_input)\n",
    "          true_output = y_test[j:(j+1)]\n",
    "          \n",
    "          # Calculate the save the loss, but don't update the model\n",
    "          loss = criterion(train_output, true_output)\n",
    "          epoch_losses_test.append(loss)\n",
    "\n",
    "      # Save the losses\n",
    "      losses_train.append((sum(epoch_losses_train) / len(epoch_losses_train)).sqrt())\n",
    "      losses_test.append((sum(epoch_losses_test) / len(epoch_losses_test)).sqrt())\n",
    "\n",
    "      # Allow model to train on the next iterations\n",
    "      rnn_model.train(True)\n",
    "    \n",
    "    else:\n",
    "\n",
    "      # Go across the entire dataset\n",
    "      for j in range(0, int(X_train.shape[0])):\n",
    "      \n",
    "          # Zero the  gradient\n",
    "          rnn_optimizer.zero_grad()\n",
    "\n",
    "          # Select inputs\n",
    "          train_input = X_train[j:(j+1),:, :]\n",
    "          train_input = X_train[(j):(j+1),:((np.where(train_input.sum(axis=2).cpu().numpy() != -300))[1]).max(), :]\n",
    "          train_input = train_input.reshape(train_input.shape[1], 1, train_input.shape[2])\n",
    "\n",
    "          # Skip 0 length sequences\n",
    "          if train_input.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "          # Get hte training output\n",
    "          train_output = rnn_model(train_input)\n",
    "          true_output = y_train[j:(j+1)]\n",
    "          \n",
    "          # Calculate the loss and update the model\n",
    "          loss = criterion(train_output, true_output)\n",
    "          loss.backward()\n",
    "          rnn_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "rQYop2ahOrx5",
    "outputId": "84a1ea3e-0bd5-4021-8544-211528c0f72b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18b05970508>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training /testing losses\n",
    "plt.plot(np.arange(1, 100)) #losses_train_plot)\n",
    "plt.plot(np.arange(5, 105)) #losses_test_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3OAGoPsDhQc"
   },
   "source": [
    "Evaluate the model on both training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "P453LRPnOsfO",
    "outputId": "17f45e35-3e83-4810-8ab7-148641ba7dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8324, device='cuda:0')\n",
      "tensor(0.8260, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Turn training off\n",
    "rnn_model.train(False)\n",
    "\n",
    "# Track accuracies\n",
    "accuracy_train = []\n",
    "for j in range(0, int(X_train.shape[0])):\n",
    "\n",
    "    # Select inputs\n",
    "    train_input = X_train[j:(j+1),:, :]\n",
    "    train_input = X_train[(j):(j+1),:((np.where(train_input.sum(axis=2).cpu().numpy() != -300))[1]).max(), :]\n",
    "    train_input = train_input.reshape(train_input.shape[1], 1, train_input.shape[2])\n",
    "\n",
    "    # Skip if 0 length\n",
    "    if train_input.shape[0] == 0:\n",
    "      continue\n",
    "\n",
    "    # Calculate hte model output\n",
    "    train_output = rnn_model(train_input)\n",
    "    true_output = y_train[j:(j+1)]\n",
    "\n",
    "    # Check if it was correct\n",
    "    correct = (train_output > 0.5) == true_output\n",
    "    accuracy_train.append(correct.float())\n",
    "\n",
    "# Do same on testing data\n",
    "accuracy_test = []\n",
    "for j in range(0, int(X_test.shape[0])):\n",
    "\n",
    "    # Select inputs\n",
    "    train_input = X_test[j:(j+1),:, :]\n",
    "    train_input = X_test[(j):(j+1),:((np.where(train_input.sum(axis=2).cpu().numpy() != -300))[1]).max(), :]\n",
    "    train_input = train_input.reshape(train_input.shape[1], 1, train_input.shape[2])\n",
    "\n",
    "    if train_input.shape[0] == 0:\n",
    "      continue\n",
    "\n",
    "    if train_input.shape[0] == 0:\n",
    "      continue\n",
    "\n",
    "    train_output = rnn_model(train_input)\n",
    "    true_output = y_train[j:(j+1)]\n",
    "\n",
    "    correct = (train_output > 0.5) == true_output\n",
    "    accuracy_test.append(correct.float())\n",
    "\n",
    "# Calculate overall accuracies\n",
    "print((sum(accuracy_train)[0][0]) / len(accuracy_train))\n",
    "print((sum(accuracy_test)[0][0]) / len(accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3uIuM2mmHAO_",
    "outputId": "7feea6c9-badc-46dd-f3e3-48b7542aed20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8200], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracies if only guessed 0\n",
    "accuracy_naive = []\n",
    "for j in range(0, int(X_test.shape[0])):\n",
    "\n",
    "    train_output = 0\n",
    "    true_output = y_test[j:(j+1)]\n",
    "    \n",
    "    loss = (train_output > 0.5) == true_output\n",
    "    accuracy_naive.append(loss.float())\n",
    "print(sum(accuracy_naive) / len(accuracy_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y84peZo6HP0z"
   },
   "source": [
    "Build an MLP for the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5mMAgb9BE7R"
   },
   "outputs": [],
   "source": [
    "# Define the network as a class for convenience\n",
    "class LinearNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearNetwork, self).__init__()\n",
    "        \n",
    "        # 5 linear layers\n",
    "        self.linear1 = torch.nn.Linear(input_size, 1024)\n",
    "        self.linear2 = torch.nn.Linear(1024, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 64)\n",
    "        self.linear4 = torch.nn.Linear(64, 16)\n",
    "        self.linear5 = torch.nn.Linear(16, 1)\n",
    "\n",
    "        # Use sigmoid activation between each layer\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "        \n",
    "    # Performs the forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.linear1(x)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.linear2(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.linear3(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.linear4(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.linear5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klkG94Z3HXRT"
   },
   "source": [
    "For bag of words, collapse along sequence axis (will throw memory error otherwise). If not, just reeshape to a 2 dimensional matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IiHJa6sl517w"
   },
   "outputs": [],
   "source": [
    "if dataset == \"BOW\":\n",
    "  X_train = X_train.sum(axis=1)\n",
    "  X_test = X_test.sum(axis=1)\n",
    "else:\n",
    "  X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "  X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uV8eq711HibQ"
   },
   "outputs": [],
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMWhkjKcamTD"
   },
   "outputs": [],
   "source": [
    "# Defines a model and prepare for training\n",
    "linear_model = LinearNetwork(X_train.shape[1])\n",
    "linear_model.to('cuda')\n",
    "linear_model.train(True)\n",
    "linear_model.float()\n",
    "\n",
    "# Use the mse loss for a criterion\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the adam optimizer on the discriminator network\n",
    "linear_optimizer = torch.optim.Adam(linear_model.parameters(), lr=0.001,\n",
    "                                 betas=(0.9, 0.9999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "colab_type": "code",
    "id": "BjLR3W3u1Xpq",
    "outputId": "7b01b449-5ca0-45bc-c8ef-3f296325ea1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  of  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10  of  100\n",
      "20  of  100\n",
      "30  of  100\n",
      "40  of  100\n",
      "50  of  100\n",
      "60  of  100\n",
      "70  of  100\n",
      "80  of  100\n",
      "90  of  100\n",
      "100  of  100\n"
     ]
    }
   ],
   "source": [
    "# Save the losses\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "epochs = 100\n",
    "\n",
    "# Use  100 epochs\n",
    "for i in range(0, epochs+1):\n",
    "    \n",
    "    # Every tenth iteration\n",
    "    if (i % 10 == 0):\n",
    "\n",
    "      # Track the epoch count\n",
    "      print(i, \" of \", epochs)\n",
    "\n",
    "      # Get loss on the training data\n",
    "      epoch_losses_train = []\n",
    "      for j in range(0, int(X_train.shape[0])):\n",
    "      \n",
    "          # Zero the  gradient\n",
    "          linear_optimizer.zero_grad()\n",
    "\n",
    "          # Select inputs\n",
    "          train_input = X_train[j:(j+1),:]\n",
    "          \n",
    "          # Skip this input if needed\n",
    "          if train_input.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "          # Get outputs and truth\n",
    "          train_output = linear_model(train_input)\n",
    "          true_output = y_train[j:(j+1)]\n",
    "          \n",
    "          # Calculate the loss and update the model\n",
    "          loss = criterion(train_output, true_output)\n",
    "          loss.backward()\n",
    "          linear_optimizer.step()\n",
    "\n",
    "          # Save the loss for this training iteration\n",
    "          epoch_losses_train.append(loss)\n",
    "\n",
    "      # Set training to false\n",
    "      linear_model.train(False)\n",
    "\n",
    "      # Get loss on the testing data\n",
    "      epoch_losses_test = []\n",
    "      for j in range(0, int(X_test.shape[0])):\n",
    "\n",
    "          # Select inputs\n",
    "          train_input = X_test[j:(j+1),:]\n",
    "          #train_input = train_input.reshape(1, train_input.shape[1] * train_input.shape[2])\n",
    "\n",
    "          # Sip if neceeded\n",
    "          if train_input.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "          # Get outputs and truth\n",
    "          train_output = linear_model(train_input)\n",
    "          true_output = y_test[j:(j+1)]\n",
    "          \n",
    "          # Calculate the save the loss, but don't update the model\n",
    "          loss = criterion(train_output, true_output)\n",
    "          epoch_losses_test.append(loss)\n",
    "\n",
    "      # Save the losses\n",
    "      losses_train.append((sum(epoch_losses_train) / len(epoch_losses_train)).sqrt())\n",
    "      losses_test.append((sum(epoch_losses_test) / len(epoch_losses_test)).sqrt())\n",
    "\n",
    "      # Allow model to train on the next iterations\n",
    "      linear_model.train(True)\n",
    "    \n",
    "    else:\n",
    "\n",
    "      # Go across the entire dataset\n",
    "      for j in range(0, int(X_train.shape[0])):\n",
    "      \n",
    "          # Zero the  gradient\n",
    "          linear_optimizer.zero_grad()\n",
    "\n",
    "          # Select inputs\n",
    "          train_input = X_train[j:(j+1),:]\n",
    "          #train_input = train_input.reshape(1, train_input.shape[1] * train_input.shape[2])\n",
    "\n",
    "          if train_input.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "          train_output = linear_model(train_input)\n",
    "          true_output = y_train[j:(j+1)]\n",
    "          \n",
    "          loss = criterion(train_output, true_output)\n",
    "          loss.backward()\n",
    "          linear_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJzn7JOGLLp9"
   },
   "source": [
    "Also evaluate this model on the training / testing datasets, using the same method as the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "EkxQDYLkbKIA",
    "outputId": "cbcae17d-8981-4e76-a564-71b37750f2a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8324, device='cuda:0')\n",
      "tensor(0.8260, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "linear_model.train(False)\n",
    "\n",
    "accuracy_train = []\n",
    "for j in range(0, int(X_train.shape[0])):\n",
    "\n",
    "    # Select inputs\n",
    "    train_input = X_train[j:(j+1),:]\n",
    "\n",
    "    if train_input.shape[0] == 0:\n",
    "      continue\n",
    "\n",
    "    train_output = linear_model(train_input)\n",
    "    true_output = y_train[j:(j+1)]\n",
    "\n",
    "    correct = (train_output > 0.5) == true_output\n",
    "    accuracy_train.append(correct.float())\n",
    "\n",
    "accuracy_test = []\n",
    "for j in range(0, int(X_test.shape[0])):\n",
    "\n",
    "    # Select inputs\n",
    "    train_input = X_test[j:(j+1),:]\n",
    "\n",
    "    if train_input.shape[0] == 0:\n",
    "      continue\n",
    "\n",
    "    train_output = linear_model(train_input)\n",
    "    true_output = y_train[j:(j+1)]\n",
    "\n",
    "    correct = (train_output > 0.5) == true_output\n",
    "    accuracy_test.append(correct.float())\n",
    "\n",
    "print((sum(accuracy_train)[0][0]) / len(accuracy_train))\n",
    "print((sum(accuracy_test)[0][0]) / len(accuracy_test))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "6_AllModels.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
